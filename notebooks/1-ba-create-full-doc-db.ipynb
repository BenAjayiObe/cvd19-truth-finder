{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "\n",
    "import json\n",
    "import dask.bag as db\n",
    "from dask import dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bajayi-obe/Documents/personal_git/cvd19-truth-finder/notebooks\n"
     ]
    }
   ],
   "source": [
    "# lets know where we are\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DOC_PATH_BIORXIV_MEDRXIV = \"../cvd19-documents/biorxiv_medrxiv/biorxiv_medrxiv\"\n",
    "RAW_DOC_PATH_COMM_USE_SUBSET = \"../cvd19-documents/comm_use_subset/comm_use_subset\"\n",
    "RAW_DOC_PATH_CUSTOM_LICENSE = \"../cvd19-documents/custom_license/custom_license\"\n",
    "RAW_DOC_PATH_NONCOMM_USE_SUBSET = \"../cvd19-documents/noncomm_use_subset/noncomm_use_subset\"\n",
    "\n",
    "PROCESSED_DOC_PATH = \"../data/covid19/pre-database-documents/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pre-Processed Files from papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_files(path):\n",
    "    \"\"\"Walk through all files located under a root path.\"\"\"\n",
    "    if os.path.isfile(path):\n",
    "        yield path\n",
    "    elif os.path.isdir(path):\n",
    "        for dirpath, _, filenames in os.walk(path):\n",
    "            for f in filenames:\n",
    "                yield os.path.join(dirpath, f)\n",
    "    else:\n",
    "        raise RuntimeError('Path %s is invalid' % path)\n",
    "        \n",
    "def collect_text(text):\n",
    "    \"\"\"Combing list of text elements into one.\"\"\"\n",
    "    return \" \".join(regex_filter(seg['text']) for seg in text)\n",
    "\n",
    "def regex_filter(text):\n",
    "    \"\"\"Removing numbers and puncutation.\"\"\"\n",
    "    text = re.sub(r\" \\d+\", \"\",text)\n",
    "    return re.sub(r\"[^A-Za-z0-9 -]+\", \"\",text)\n",
    "\n",
    "def clean_json_read(json_file_name):\n",
    "    \"\"\"Convert multi-line json files into one large file of single line jsons.\"\"\"\n",
    "    \n",
    "    # reading in document\n",
    "    document = open(json_file_name).read()\n",
    "\n",
    "    # removing new lines and whitespace\n",
    "    document = document.replace(\"\\n\", \"\")\n",
    "    \n",
    "    document_obj = json.loads(document)\n",
    "\n",
    "    return document_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting jsons into single files for easier processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all json papers in directory\n",
    "BIORXIV_MEDRXIV_JSON_PATH_LIST = list(iter_files(RAW_DOC_PATH_BIORXIV_MEDRXIV))\n",
    "COMM_USE_SUBSET_JSON_PATH_LIST = list(iter_files(RAW_DOC_PATH_COMM_USE_SUBSET))\n",
    "CUSTOM_LICENSE_JSON_PATH_LIST = list(iter_files(RAW_DOC_PATH_CUSTOM_LICENSE))\n",
    "NONCOMM_USE_SUBSET_JSON_PATH_LIST = list(iter_files(RAW_DOC_PATH_NONCOMM_USE_SUBSET))\n",
    "\n",
    "JSON_DICT = {\"biorxiv_medrxiv\": BIORXIV_MEDRXIV_JSON_PATH_LIST,\n",
    "             \"comm_use_subset\": COMM_USE_SUBSET_JSON_PATH_LIST,\n",
    "             \"comm_license\": CUSTOM_LICENSE_JSON_PATH_LIST,\n",
    "             \"noncomm_use_subset\": NONCOMM_USE_SUBSET_JSON_PATH_LIST}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting\n",
    "def combine_all_jsons(category):\n",
    "    json_file_name = JSON_DICT[category]\n",
    "    with open(f\"../data/covid19/pre-database-documents/{category}.json\", \"w\") as f:\n",
    "        for json_path in tqdm(json_file_name):    \n",
    "            f.write(json.dumps(clean_json_read(json_path)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1054/1054 [00:01<00:00, 653.44it/s]\n",
      "100%|██████████| 9315/9315 [00:31<00:00, 296.94it/s]\n",
      "100%|██████████| 20657/20657 [01:20<00:00, 256.34it/s]\n",
      "100%|██████████| 2350/2350 [00:06<00:00, 339.23it/s]\n"
     ]
    }
   ],
   "source": [
    "combine_all_jsons(\"biorxiv_medrxiv\")\n",
    "\n",
    "combine_all_jsons(\"comm_use_subset\")\n",
    "\n",
    "combine_all_jsons(\"comm_license\")\n",
    "\n",
    "combine_all_jsons(\"noncomm_use_subset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting all json text's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_json_files = db.read_text(\"../data/covid19/pre-database-documents/*.json\").map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_text(record):\n",
    "    \n",
    "    return {\"paper_id\": record[\"paper_id\"],\n",
    "            \"title\": record[\"metadata\"][\"title\"],\n",
    "            \"abstract\": [abst[\"text\"] + abst[\"section\"]\n",
    "                         for abst in record[\"abstract\"]],\n",
    "            \"body\": [abst[\"text\"] + abst[\"section\"]\n",
    "                         for abst in record[\"body_text\"]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_paper_df = d_json_files.map(collect_text).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33376"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_paper_df.shape[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting text from json papers\n",
    "d_paper_df[\"text\"] = d_paper_df.apply(lambda row: \" \".join(row[\"abstract\"]) + \\\n",
    "                                                           \" \".join(row[\"body\"]), meta=(None, 'object'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'67e82c72-6d7a-4618-b726-c7b9577e78af'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uuid.uuid4().__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a unique id\n",
    "d_paper_df[\"id\"] = d_paper_df.apply(lambda x: uuid.uuid4().__str__(), \n",
    "                                    meta=(None, 'object'), \n",
    "                                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/bajayi-obe/Documents/personal_git/cvd19-truth-finder/data/covid19/pre-database-documents/processed/processed.0.json',\n",
       " '/Users/bajayi-obe/Documents/personal_git/cvd19-truth-finder/data/covid19/pre-database-documents/processed/processed.1.json',\n",
       " '/Users/bajayi-obe/Documents/personal_git/cvd19-truth-finder/data/covid19/pre-database-documents/processed/processed.2.json',\n",
       " '/Users/bajayi-obe/Documents/personal_git/cvd19-truth-finder/data/covid19/pre-database-documents/processed/processed.3.json']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writing id and text to file\n",
    "d_paper_df[[\"id\", \"text\"]].to_bag()\\\n",
    "                                .map(lambda x: {\"id\": x[0], \"text\": x[1]})\\\n",
    "                                .map(json.dumps)\\\n",
    "                                .to_textfiles(\n",
    "                '../data/covid19/pre-database-documents/processed/processed.*.json'\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an SQLitte Database from Pre-Processed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"../scripts/retriever/build_db.py\", line 18, in <module>\n",
      "    from drqa.retriever import utils\n",
      "ModuleNotFoundError: No module named 'drqa'\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/retriever/build_db.py \\\n",
    "\"../data/covid19/pre-database-documents/processed\" \\\n",
    "\"../data/covid19/doc-db/full_doc.db\" \\\n",
    "--num-workers=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TF-IDF Model from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/09/2020 02:16:47 PM: [ Counting words... ]\n",
      "05/09/2020 02:16:47 PM: [ Mapping... ]\n",
      "05/09/2020 02:16:47 PM: [ -------------------------Batch 1/4------------------------- ]\n",
      "05/09/2020 02:16:47 PM: [ -------------------------Batch 2/4------------------------- ]\n",
      "05/09/2020 02:16:47 PM: [ -------------------------Batch 3/4------------------------- ]\n",
      "05/09/2020 02:16:47 PM: [ -------------------------Batch 4/4------------------------- ]\n",
      "05/09/2020 02:16:47 PM: [ Creating sparse matrix... ]\n",
      "05/09/2020 02:16:47 PM: [ Making tfidf vectors... ]\n",
      "05/09/2020 02:16:49 PM: [ Getting word-doc frequencies... ]\n",
      "05/09/2020 02:16:50 PM: [ Saving to /home/ubuntu/cvd19-truth-finder/data/covid19/doc_ranker/doc-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz ]\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/retriever/build_tfidf.py \\\n",
    "\"../data/covid19/doc-db/full_doc.db\" \\\n",
    "\"../data/covid19/full_doc_ranker\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process SQUAD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset /home/ubuntu/cvd19-truth-finder/drQA-training-datasets/datasets/SQuAD-v1.1-train.json\n",
      "Will write to file /home/ubuntu/cvd19-truth-finder/drQA-training-datasets/datasets/SQuAD-v1.1-train-processed-spacy.txt\n",
      "Total time: 142.2876 (s)\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/reader/preprocess.py \\\n",
    "\"/home/ubuntu/cvd19-truth-finder/drQA-training-datasets/datasets\" \\\n",
    "\"/home/ubuntu/cvd19-truth-finder/drQA-training-datasets/datasets\" \\\n",
    "--split SQuAD-v1.1-train \\\n",
    "--tokenizer spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset /home/ubuntu/cvd19-truth-finder/drQA-training-datasets/datasets/SQuAD-v1.1-dev.json\n",
      "Will write to file /home/ubuntu/cvd19-truth-finder/drQA-training-datasets/datasets/SQuAD-v1.1-dev-processed-spacy.txt\n",
      "Total time: 21.2823 (s)\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/reader/preprocess.py \\\n",
    "\"/home/ubuntu/cvd19-truth-finder/drQA-training-datasets/datasets\" \\\n",
    "\"/home/ubuntu/cvd19-truth-finder/drQA-training-datasets/datasets\" \\\n",
    "--split SQuAD-v1.1-dev \\\n",
    "--tokenizer spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/reader/train.py \\\n",
    "--num-epochs=1 \\\n",
    "--model-dir=\"/home/ubuntu/cvd19-truth-finder/data/covid19/model\" \\\n",
    "--data-dir=\"/home/ubuntu/cvd19-truth-finder/drQA-training-datasets/datasets\" \\\n",
    "--train-file=\"/home/ubuntu/cvd19-truth-finder/drQA-training-datasets/datasets/SQuAD-v1.1-train-processed-spacy.txt\" \\\n",
    "--dev-file=\"/home/ubuntu/cvd19-truth-finder/drQA-training-datasets/datasets/SQuAD-v1.1-dev-processed-spacy.txt\" \\\n",
    "--embed-dir=\"/home/ubuntu/cvd19-truth-finder/drQA-training-datasets/datasets/\" \\\n",
    "--embedding-file=\"glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Predictions:\n",
      "+------+--------+-----+--------------+-----------+\n",
      "| Rank | Answer | Doc | Answer Score | Doc Score |\n",
      "+------+--------+-----+--------------+-----------+\n",
      "+------+--------+-----+--------------+-----------+\n",
      "\n",
      "Contexts:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import code\n",
    "import prettytable\n",
    "\n",
    "from termcolor import colored\n",
    "from drqa import pipeline\n",
    "from drqa.retriever import utils\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Arguments\n",
    "    args_reader_model=\"../data/covid19/model/20200509-c9d21e14.mdl\"\n",
    "    args_retriever_model=\"../data/covid19/full_doc_ranker/full_doc-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz\"\n",
    "    args_doc_db=\"../data/covid19/doc-db/full_doc.db\"\n",
    "    args_tokenizer=\"spacy\"\n",
    "    args_candidate_file=None\n",
    "    args_no_cuda=True\n",
    "    args_gpu=-1\n",
    "\n",
    "    args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
    "    if args_cuda:\n",
    "        torch.cuda.set_device(args_gpu)\n",
    "\n",
    "    if args_candidate_file:\n",
    "        candidates = set()\n",
    "        with open(args_candidate_file) as f:\n",
    "            for line in f:\n",
    "                line = utils.normalize(line.strip()).lower()\n",
    "                candidates.add(line)\n",
    "    else:\n",
    "        candidates = None\n",
    "\n",
    "    DrQA = pipeline.DrQA(\n",
    "        cuda=args_cuda,\n",
    "        fixed_candidates=candidates,\n",
    "        reader_model=args_reader_model,\n",
    "        ranker_config={'options': {'tfidf_path': args_retriever_model}},\n",
    "        db_config={'options': {'db_path': args_doc_db}},\n",
    "        tokenizer=args_tokenizer\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Drop in to interactive mode\n",
    "    # ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def process(question, candidates=None, top_n=5, n_docs=5):\n",
    "        predictions = DrQA.process(\n",
    "            question, candidates, top_n, n_docs, return_context=True\n",
    "        )\n",
    "        table = prettytable.PrettyTable(\n",
    "            ['Rank', 'Answer', 'Doc', 'Answer Score', 'Doc Score']\n",
    "        )\n",
    "        for i, p in enumerate(predictions, 1):\n",
    "            table.add_row([i, p['span'], p['doc_id'],\n",
    "                           '%.5g' % p['span_score'],\n",
    "                           '%.5g' % p['doc_score']])\n",
    "        print('Top Predictions:')\n",
    "        print(table)\n",
    "        print('\\nContexts:')\n",
    "        for p in predictions:\n",
    "            text = p['context']['text']\n",
    "            start = p['context']['start']\n",
    "            end = p['context']['end']\n",
    "            output = (text[:start] +\n",
    "                      colored(text[start: end], 'green', attrs=['bold']) +\n",
    "                      text[end:])\n",
    "            print('[ Doc = %s ]' % p['doc_id'])\n",
    "            print(output + '\\n')\n",
    "\n",
    "    process(\"virus\",n_docs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv19_env",
   "language": "python",
   "name": "cv19_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
